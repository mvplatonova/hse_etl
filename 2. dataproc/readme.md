1. Подготовлена инфраструктура по [инструкции](https://yandex.cloud/ru/docs/managed-airflow/tutorials/data-processing-automation#infra)
    - S3 бакет
    - Metastore кластер
    - Airflow кластер
2. Данные были сгенерированы скриптом из первого задания (сам скрипт и csv файл лежат в папке "1. ydb_s3")
3. PySpark задание (файл analyse.py) считает ежедневную выручку по категориям товаров и записывает данные в s3
4. В файле etl-dag.py находится dag DATA_INGEST, который создает кластер, запускает задание, удаляет кластер
5. В папке result лежит скриншот успешного запуска DAG, а также скриншот и файл полученного результата в s3 
